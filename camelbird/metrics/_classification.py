"""Metrics to assess fairness on classification task given class prediction and group membership

"""
# Authors: Hilde Weerts
# License: BSD 3 clause

import numpy as np

from sklearn.utils import column_or_1d
from sklearn.utils.validation import _check_sample_weight
from sklearn.utils.multiclass import type_of_target

from sklearn.metrics import recall_score


def _check_binary(x):
    xtype = type_of_target(x)
    return xtype == 'binary'


def _diff(scores):
    """Compute difference between scores.

    Parameters
    ----------
    scores : array of size 2

    Returns
    -------
    diff : float
    """
    if len(scores) != 2:
        raise ValueError("Non-binary sensitive feature is currently not supported.")
    diff = scores[0] - scores[1]
    return diff


def _ratio(scores):
    """Compute ratio of scores.

    Parameters
    ----------
    scores : array of size 2

    Returns
    -------
    diff : float
    """
    if len(scores) != 2:
        raise ValueError("Non-binary sensitive feature is currently not supported.")
    ratio = scores[1] / scores[0]
    return ratio


def _score_subgroups(y_true, y_pred, a, metric, sample_weight=None, **metric_params):
    """Compute scores for each subgroup according to a (scikit-learn style) metric.

    Samples will be weighted relative to their sample_weight within each subgroup separately.

    Parameters
    ----------
    y_true : 1d array-like
        Ground truth (correct) target values.

    y_pred : 1d array-like
        Estimated targets as returned by a classifier.

    a : 1d array-like
        Sensitive group membership.

    metric : callable
        Metric that must be computed for each group. Callable of form metric(y_true, y_pred, sample_weight, **kwargs).

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    Returns
    -------
    scores : array of shape (n_subgroups,)
        Computed scores for each group.
    """
    y_true = column_or_1d(y_true)
    y_pred = column_or_1d(y_pred)
    a = column_or_1d(a)
    sample_weight = _check_sample_weight(sample_weight, y_true)

    if not _check_binary(y_true) or not _check_binary(y_pred):
        raise NotImplementedError("Non-binary target is currently not supported.")
    if not _check_binary(a):
        raise NotImplementedError("Non-binary sensitive feature is currently not supported.")

    subgroups = np.unique(a)
    n_subgroups = len(subgroups)
    scores = np.empty((n_subgroups,))
    for subgroup, i in zip(subgroups, range(n_subgroups)):
        cond = a == subgroup
        scores[i] = metric(y_true=y_true[cond], y_pred=y_pred[cond], sample_weight=sample_weight[cond],
                           **metric_params)
    return np.array(scores)


def _base_rate_score(y_true, y_pred, sample_weight):
    """Compute base rate of predictions.

    Parameters
    ----------
    y_true  : ignored
        Not used, present here for API consistency by convention.

    y_pred : array-like of shape (n_samples,)
        Predicted labels as integers, ``1`` for positive class, ``0`` for negative class.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    Returns
    -------
    base_rate : float
        Base rate of predictions.

    """
    y_pred = column_or_1d(y_pred)
    sample_weight = _check_sample_weight(sample_weight, y_pred)

    if not _check_binary(y_pred):
        raise NotImplementedError("Non-binary target is currently not supported.")

    return sum(y_pred*sample_weight)/sum(sample_weight)


def equal_opportunity(y_true, y_pred, a, aggregate=None, sample_weight=None):
    """
    Equal opportunity fairness metric.

    Equal opportunity requires that the true positive rates, `tpr = tp / (tp + fn)` of all subgroups are equal, where
    `tp` is the number of true positives and `fn` the number of false negatives. The true positive rate is also known as
    recall. In other words, the classifier should predict the *preferred* class equally well, regardless of
    sensitive group membership. Note that this fairness metric does not take into account correct classification of true
    negatives.

    For example, consider a job hiring scenario in which we assume that getting hired is the preferred target outcome
    and gender is the sensitive feature of interest (which we will assume to be binary for ease of presentation). Equal
    opportunity requires that women who possess the abilities to do well on a job (i.e. `positives`) are just as often
    classified correctly as men who possess the ability to do well on a job.

    Parameters
    ----------
    y_true : array-like of shape (n_samples,)
        Ground truth (correct) labels as integers.

    y_pred : array-like of shape (n_samples,)
        Predicted labels as integers.

    a : array-like of shape (n_samples,)
        Sensitive group membership

    aggregate : string, [None, 'diff', 'ratio']
        The type of aggregation performed on the true positive rates.

        ``None``:
            Report the score for each group.
        ``'diff'``:
            Report the difference.
        ``'ratio'``:
            Report the ratio.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    Returns
    -------
    tpr_scores : array of shape (n_groups,)
        Raw true positive rates, if ``aggregate=None``.

    equalopp : float
        Fairness metric score, if ``aggregate='diff'`` or ``aggregate="ratio"``.

    See Also
    --------
    camelbird.metrics.equal_odds: Fairness metric that also requires equal true negative rates.
    camelbird.metrics.demographic_parity: Fairness metric that requires equal base rates.

    Notes
    -----

    See [1]_ for a more detailed discussion on equal opportunity.

    ..  [1] M. Hardt, E. Price, and N. Srebro (2016). Equality of opportunity in supervised learning.  In Advances in
        Neural Information  Processing  Systems  29, pages 3315–3323.

    """
    tpr_scores = _score_subgroups(y_true, y_pred, a, recall_score, sample_weight=sample_weight, pos_label=1)
    if aggregate is None:
        return tpr_scores
    elif aggregate == 'diff':
        return _diff(tpr_scores)
    elif aggregate == 'ratio':
        return _ratio(tpr_scores)
    else:
        raise ValueError("'aggregate' must be in [None, 'diff', 'ratio'].")


def demographic_parity(y_true, y_pred, a, aggregate=None, sample_weight=None):
    """Demographic parity fairness metric.

    Demographic parity requires that the base rates (i.e. percentage of predicted positives), are equal for all
    subgroups.

    For example, consider a job hiring scenario in which we assume that getting hired is the preferred target outcome
    and gender is the sensitive feature of interest (which we will assume to be binary for ease of presentation).
    Demographic parity requires that the percentage of women that is hired is the same as the percentage of men,
    irrespective of whether one group is, on average, more qualified.

    In contrast to equal odds and equal opportunity, demographic parity does not take into account error rates;
    i.e. the fairness metric does have any requirements regarding the accuracy of a classifier.

    Demographic parity is sometimes referred to as statistical parity.

    Parameters
    ----------
    y_true  : ignored
        Not used, present here for API consistency by convention.

    y_pred : array-like of shape (n_samples,)
        Predicted labels as integers, ``1`` for preferred class, ``0`` for undesired class.

    a : array-like of shape (n_samples,)
        Sensitive group membership, ``1`` for unprivileged group, ``0`` for privileged group.

    aggregate : string, [None, 'diff', 'ratio']
        The type of aggregation performed on the base rates.

        ``None``:
            Report the score for each group.
        ``'diff'``:
            Report the difference.
        ``'ratio'``:
            Report the ratio.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    Returns
    -------
    base_rates : array of shape (n_groups,)
        Raw base rates if ``aggregate=None``

    demopar : float
        Fairness metric score if ``aggregate='diff'`` or ``aggregate="ratio"``

    See Also
    --------
    camelbird.metrics.equal_opportunity : Fairness metric that requires equal true positive rates.
    camelbird.metrics.equal_odds : Fairness metric that requires equal true positive rates and
        equal true negative rates.

    Notes
    -----

    See [1]_  and [2]_ for a more detailed discussion on demographic parity.

    ..  [1] M. Hardt, E. Price, and N. Srebro (2016). Equality of opportunity in supervised learning.  In Advances in
        Neural Information  Processing  Systems  29, pages 3315–3323.

    ..  [2] Dwork, C., Hardt, M., Pitassi, T., Reingold, O. and Zemel, R. (2012). Fairness through awareness. In
        Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS ’12).

    """
    y_true = _check_sample_weight(y_true, y_pred)
    base_rates = _score_subgroups(y_true, y_pred, a, _base_rate_score, sample_weight=sample_weight)
    if aggregate is None:
        return base_rates
    elif aggregate == 'diff':
        return _diff(base_rates)
    elif aggregate == 'ratio':
        return _ratio(base_rates)
    else:
        raise ValueError("'aggregate' must be in [None, 'diff', 'ratio'].")


def equal_odds(y_true, y_pred, a, aggregate=None, sample_weight=None):
    """Equal odds fairness metric.

    Equal odds requires that the true positive rates, `tpr = tp / (tp + fn)`, as well as the true negative rates,
    `tnr = tn / (tn + fp)` of all subgroups are equal, where `tp` is the number of true positives, `fn` the
    number of false negatives, `tn` the number of true negatives, and `fp` the number of false positives. The `tpr`
    is also known as recall. The `tnr` is also equal to `1 - fpr`, where fpr is the false positive rate.

    For example, consider a job hiring scenario in which we assume that getting hired is the preferred target outcome
    and gender is the sensitive feature of interest (which we will assume to be binary for ease of presentation).
    Equal odds requires that

    Parameters
    ----------
    y_true : array-like of shape (n_samples,)
        Ground truth (correct) labels as integers.

    y_pred : array-like of shape (n_samples,)
        Predicted labels as integers.

    a : array-like of shape (n_samples,)
        Sensitive group membership

    aggregate : string, [None, 'diff', 'ratio']
        The type of aggregation performed on the true positive rates and true negative rates.

        ``None``:
            Report the `tpr` and `tnr` for each group.
        ``'diff'``:
            Report the average of `tpr` difference between groups and `tnr` difference between groups.
        ``'ratio'``:
            Report the average of `tpr` ratio between groups and `tnr` ratio between groups.

    sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.

    Returns
    -------
    scores : array of shape (n_groups,2)
        Raw true positive rates and true negative rates, if ``aggregate=None``.

    equalodd : float
        If ``aggregate='diff'``, returns the average of the `tpr` difference and `tnr` difference.
        If ``aggregate="ratio"``, returns the average of the `tpr` ratio and the `tnr` ratio.

    See Also
    --------
    camelbird.metrics.equal_opportunity : Fairness metric that requires only equal true positive rates.
    camelbird.metrics.demographic_parity : Fairness metric that requires equal base rates.

    Notes
    -----

    See [1]_ for a more detailed discussion on equal odds.

    ..  [1] M. Hardt, E. Price, and N. Srebro (2016). Equality of opportunity in supervised learning.  In Advances in
        Neural Information  Processing  Systems  29, pages 3315–3323.

    """
    tpr_scores = _score_subgroups(y_true, y_pred, a, recall_score, sample_weight=sample_weight, pos_label=1)
    tnr_scores = _score_subgroups(y_true, y_pred, a, recall_score, sample_weight=sample_weight, pos_label=0)
    if aggregate is None:
        return np.array([tpr_scores, tnr_scores])
    elif aggregate == 'diff':
        diff_tpr = _diff(tpr_scores)
        diff_tnr = _diff(tnr_scores)
        return (diff_tpr + diff_tnr)/2
    elif aggregate == 'ratio':
        ratio_tpr = _ratio(tpr_scores)
        ratio_tnr = _ratio(tnr_scores)
        return (ratio_tpr + ratio_tnr)/2
    else:
        raise ValueError("'aggregate' must be in [None, 'diff', 'ratio'].")
